<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
    2.1 General Expectation Maximation Algorithm &middot; Inference
    
  </title>

  <script type="text/javascript">
    document.addEventListener('DOMContentLoaded', function () {
      document.querySelectorAll("script[type='math/tex']").forEach(function (el) {
        el.outerHTML = "\\(" + el.textContent + "\\)";
      });
      document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function (el) {
        el.outerHTML = "\\[" + el.textContent + "\\]";
      });
      var script = document.createElement('script');
      script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js";
      document.head.appendChild(script);
    }, false);
  </script>

  
  <link rel="canonical" href="/em/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144"
    href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
</head>

<body class="theme-base-0d">

  <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Bayesian Methods for Machine Learning</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item"
      href="/">Overview</a>

    

    
    
    
    
    <a class="sidebar-nav-item"
      href="/ei/">1. Bayesian Inference</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/mle/">1.1 Maximum Likelihood Estimation</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/reg/">1.2 Bayesian Regularization</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/lvmod/">2. Latent Variable Models</a>
    
    
    
    
    
    <a class="sidebar-nav-item active"
      href="/em/">2.1 General Expectation Maximation Algorithm</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/gmm/">2.2 Gaussian Mixture Models</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/vi/">3. Variational Inference</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/vfe/">3.1 Variational Free Energy</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/gp/">4. Gaussian Processes</a>
    
    
    
    
    

    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span>
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>

  <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
  <div class="wrap">
    <div class="masthead">
      <div class="container">
        <h3 class="masthead-title">
          <a href="/" title="Home">Inference</a>
          <small>Machine Learning</small>
        </h3>
      </div>
    </div>

    <div class="container content">
      <div class="page">
  <h1 class="page-title">2.1 General Expectation Maximation Algorithm</h1>
  <ul id="markdown-toc">
  <li><a href="#the-em-algorithm" id="markdown-toc-the-em-algorithm">The EM-algorithm</a></li>
  <li><a href="#update-formulas" id="markdown-toc-update-formulas">Update Formulas</a></li>
</ul>
<p>During the training of a model, the parameter <script type="math/tex">\theta</script> is tuned to maximize the likelihood <script type="math/tex">p(X\|\theta)</script> of the observed dataset.</p>

<p>For i.i.d. data points the loglikelihood factorizes. Using the chain rule, we introduce a latent variables <script type="math/tex">t_i</script></p>

<script type="math/tex; mode=display">\log p(X|\theta) = \sum_i log p(x_i|\theta) = \sum_{i} \log \sum_{c}p(x_i, t_i=c| \theta)</script>

<p>The advantage of the EM algorithm is to maximize a lower bound instead of the complicated loglikelihood <script type="math/tex">p(X\|\theta)</script>. To find a lower bound we make use of the <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jenson inequality</a>. But first, we need to transform the argument of the logarithm by inserting <script type="math/tex">1 = \frac{q(t_i=c)}{q(t_i=c)}</script></p>

<script type="math/tex; mode=display">\log p(X|\theta) = \sum_{i} \log \sum_{c}q(t_i=c) \frac{p(x_i, t_i=c| \theta)}{q(t_i=c)} = \sum_i \log \left \langle \frac{p(X,t_i|\theta)}{q(t_i)} \right\rangle_{q(t)}</script>

<p>By this trick, Jenson inequality is applicable and we find a lower bound <script type="math/tex">\mathcal L(\theta, q)</script></p>

<script type="math/tex; mode=display">\log p(X|\theta) = \sum_i \log \left \langle \frac{p(X,t_i|\theta)}{q(t_i)} \right\rangle_{q(t)} \geq \sum_i \left \langle \log \left( \frac{p(X,t_i|\theta)}{q(t_i)}\right) \right \rangle_{q(t)}</script>

<h2 id="the-em-algorithm">The EM-algorithm</h2>

<p>The lower bound <script type="math/tex">\mathcal L (\theta, q)</script> now, is maximized in two steps. In the first step, called the <strong>expectation step</strong>,  we vary <script type="math/tex">q(t_i)</script> while <script type="math/tex">\theta</script> is kept fix.</p>

<p>It can be shown, that the gap <script type="math/tex">\Delta</script> between the loglikelihood <script type="math/tex">p(X\|\theta)</script> and the lower bound <script type="math/tex">\mathcal L</script> is given by the Kullback-Leibler divergence</p>

<script type="math/tex; mode=display">\Delta = \log p(X|\theta) - \mathcal L(\theta, q) = \mathcal{KL}\left(q(t_i) || p(t_i| x_i, \theta)\right)</script>

<p>which is minimized by <script type="math/tex">q(t_i) = p(t_i\| x_i, \theta)</script>.</p>

<p>In the second step, called the <strong>maximization step</strong>, the parameter <script type="math/tex">\theta</script> is tuned to maximize the lower bound for the particular choice of <script type="math/tex">q</script></p>

<script type="math/tex; mode=display">\mathcal L(\theta, q) = \sum_i \langle \log \left(p(X,t_i|\theta)\right)\rangle_{q(t_i)} + const.</script>

<p>The second term is constant w.r.t. <script type="math/tex">\theta</script>, the first term is usually concave and thus easily maximized by gradient ascent.</p>

<h2 id="update-formulas">Update Formulas</h2>

<p><strong>E-step:</strong></p>

<script type="math/tex; mode=display">q^{k+1}(t_i) = p(t_i| x_i, \theta^k) = \frac{p(x_i|t_i, \theta^k) q^k(t_i)}{\sum_c p(x_i|t_i=c, \theta^k) q^k(t_i=c)}</script>

<p><strong>M-step:</strong></p>

<script type="math/tex; mode=display">\theta^{k+1} = \text{argmax} \sum_i \mathbb E_{q^{k+1}(t_i)} \log \left(p(x_i,t_i|\theta^k)\right)</script>


</div>
    </div>
  </div>

  <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  <script src='/public/js/script.js'></script>
</body>

</html>