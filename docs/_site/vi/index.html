<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
    3. Variational Inference &middot; Inference
    
  </title>

  <script type="text/javascript">
    document.addEventListener('DOMContentLoaded', function () {
      document.querySelectorAll("script[type='math/tex']").forEach(function (el) {
        el.outerHTML = "\\(" + el.textContent + "\\)";
      });
      document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function (el) {
        el.outerHTML = "\\[" + el.textContent + "\\]";
      });
      var script = document.createElement('script');
      script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js";
      document.head.appendChild(script);
    }, false);
  </script>

  
  <link rel="canonical" href="/vi/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144"
    href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
</head>

<body class="theme-base-0d">

  <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Bayesian Methods for Machine Learning</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item"
      href="/">Overview</a>

    

    
    
    
    
    <a class="sidebar-nav-item"
      href="/ei/">1. Bayesian Inference</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/mle/">1.1 Maximum Likelihood Estimation</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/reg/">1.2 Bayesian Regularization</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/lvmod/">2. Latent Variable Models</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/em/">2.1 General Expectation Maximation Algorithm</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/gmm/">2.2 Gaussian Mixture Models</a>
    
    
    
    
    
    <a class="sidebar-nav-item active"
      href="/vi/">3. Variational Inference</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/vfe/">3.1 Variational Free Energy</a>
    
    
    
    
    
    <a class="sidebar-nav-item"
      href="/gp/">4. Gaussian Processes</a>
    
    
    
    
    

    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span>
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>

  <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
  <div class="wrap">
    <div class="masthead">
      <div class="container">
        <h3 class="masthead-title">
          <a href="/" title="Home">Inference</a>
          <small>Machine Learning</small>
        </h3>
      </div>
    </div>

    <div class="container content">
      <div class="page">
  <h1 class="page-title">3. Variational Inference</h1>
  <ul id="markdown-toc">
  <li><a href="#variational-lower-bound-decomposition" id="markdown-toc-variational-lower-bound-decomposition">Variational Lower Bound Decomposition</a></li>
</ul>
<p>For many practical models evaluating <script type="math/tex">p(Z\|X)</script> is infeasible and approximation schemes are required. There are mainly two types of approximation schemes. The first major group consists of <strong>stochastic approximation schemes</strong> such as Markov Chain Monte Carlo, and the second major group is formed by <strong>deterministic approximation schemes</strong>.</p>

<p>In this section we will introduce a determinisitic method called <strong>variational inference</strong>.</p>

<h2 id="variational-lower-bound-decomposition">Variational Lower Bound Decomposition</h2>

<p>In variational inference, the probability distribution <script type="math/tex">p(X)</script> is approximated by a simpler distribution <script type="math/tex">q(X)</script> in two steps. First, the functional class of <script type="math/tex">q(X)</script> is reduced and afterwards we want to find the best model function <script type="math/tex">q^*(X)</script> within this class.</p>

<p>We start from a fully Bayesian model, where all parameters are stochastic with given priors. We absorbe the stochastic parameters into the latent variables <script type="math/tex">Z</script>, the no longer appear explicitly in the notation.</p>

<p>The full probability can be rewritten as the expectation value of the conditional probability <script type="math/tex">p(X\|Z)</script></p>

<script type="math/tex; mode=display">\log p(X) = \log \left( \langle p(X|Z) \rangle_{q(Z)}\right) \ge \langle \log p(X|Z) \rangle_{q(Z)}</script>

<p>where we used Jensenâ€™s inequality. By subtraction, the inequality gap turns out to be the Kullback-Leibler divergence, so we end up with the following decomposition</p>

<script type="math/tex; mode=display">\log p(X) = \mathcal L (q) + \mathcal{KL}(q||p)</script>

<p>where</p>

<script type="math/tex; mode=display">\begin{aligned}
    \mathcal L (q) = \langle \log p(X|Z) \rangle_{q(Z)}\\
    \mathcal{KL}(q||p) = - \langle \log(\frac{p(Z|X)}{q(Z)})\rangle_{q(Z)}
\end{aligned}</script>

<p>Maximizing the lower bound <script type="math/tex">\mathcal L(q)</script> w.r.t. <script type="math/tex">q</script> is equivalent to minimizing the gap, i.e. the Kullback-Leibler divergence. This is achieved by setting the prior <script type="math/tex">q(Z)</script> equal to the posterior <script type="math/tex">p(Z\|X)</script>.</p>

<p>The posterior <script type="math/tex">p(Z\| X)</script> is expected to be intractable now, so we need to start the approximation here. As mentioned above, we restrict the family of distributions <script type="math/tex">q(Z)</script>. The goal will be a restriction to a class of tractable distributions.</p>

<p>But before we present possible restrictons, we derive the variational inference from a physical perspective in the next section.</p>

</div>
    </div>
  </div>

  <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  <script src='/public/js/script.js'></script>
</body>

</html>