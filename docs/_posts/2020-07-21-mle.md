---
layout: page
title: 1.1 Maximum Likelihood Estimation
permalink: /mle/
---
* ToC
{:toc}

## Maximum Likelihood Estimation
Instead of enumerating all hypotheses, we will search for a single hypothesis from the hypothesis space $$\mathcal H$$ that fits the data well. The hypotheses in $$\mathcal H$$ are parametrized by $$\theta$$. Now, to fit the data well, we need to learn the value of $$\theta$$, such that the probability observing the data $$X$$ from the hypothesis distribution is maximized. This is summarized under the term **maximum likelihood estimation** (MLE), as we are maximizing the **likelihood** function. For convenience we will maximize the **loglikelihood** instead

$$
    \underset{\theta}{\text{argmax}} \log P(X | \theta, \mathcal H )
$$

The concept of maximum likelihood exists in the frequentist as well as in the Bayesian paradigm. While the frequentists assume a single parameter $$\theta$$ fixed by the real data distribution, the bayesians observe a single fixed dataset $$\mathcal D$$ an infer an uncertainty for the parameter $$\theta$$.

## Maximum A-Posteriori

Using Bayes rule we can calculate the uncertainty of the parameter $$\theta$$ given the observed data $$\mathcal D$$

$$
    P(\theta | X) = \frac{P(X | \theta) P(\theta)}{P(X)}
$$

However, in practice we are often interested in a single value for $$\theta$$. Two popular choices are

$$\langle \theta \rangle = \int \theta P(\theta|X) d \theta$$

$$\theta_{MAP} = \underset{\theta}{\text{argmax}} P(\theta | X)$$ 

The first expression is called **Bayes estimate** and the second one is called **Maximum A Postiori**. 