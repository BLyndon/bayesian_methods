---
layout: page
title: 2.2 Gaussian Mixture Models
permalink: /bayesian_methods/gmm/
---
* ToC
{:toc}

As a special case, a known training method for GMM is derived from the general EM principle. For GMM the likelihood is given by a weighted sum of Gaussians

$$
    p(X|\theta) = \sum_c \pi_c \mathcal N (X; \mu_c, \sigma_c)
$$

with normalized weight parameters $$\sum_c \pi_c = 1$$.

## Expectation Step

By comparison to the latent variable approach above, we establish a correspondence for each quantity from the latent variable model to the GMM. Interestingly, the latent variable $$t_i$$ has a natural interpretation as the cluster component.

$$
\begin{aligned}
    p(t_i = c) = \pi_c\\
    p(x_i | t_i = c, \theta) = \mathcal N (x; \mu_c, \Sigma_c)\\
    q^{k+1}(t_i = c) = \gamma_{ic}
\end{aligned}
$$

In the expectation step we want to minimize the Kullback-Leibler divergence by setting the prior $$q^{k1}(t_i)$$ to the posterior $$p(t_i\|x_i, \theta^k)$$. After applying the Bayes formular to the likelihood $$p(x_i | t_i = c, \theta)$$ we have

$$
    \gamma_{ic} = \frac{\pi_c \mathcal N (x_i; \mu_c, \Sigma_c)}{\sum_{c=1} \pi_c \mathcal N (x_i; \mu_c, \Sigma_c)}
$$

For numerical reasons, we rewrite the expression in the following way

$$
    \gamma_{ic} = \frac{\exp(y_{ic})}{\sum_{c=1} \exp(y_{ic})} = \frac{\exp(y_{ic} - \max(y))}{\sum_{c=1} \exp(y_{ic} - \max(y))}
$$

where $$y_{ic}$$ is given by

$$
    y_{ic} = \log \pi_c -\frac 1 2 \left(({x_i}-{\mu_c})^\mathrm{T}{\Sigma_c}^{-1}({x_i}-{\mu_c}) + d \log 2 \pi + \log \det \Sigma_c \right)
$$

## Maximation Step

In the maximization step the prior given by the expectation step is maximized w.r.t. $$\theta$$.

$$
\begin{aligned}
    \theta^{k+1} = \text{argmax} \sum_{ic} q(t_i = c) \log \left(p(x_i,t_i=c|\theta^k)\right) \\= \text{argmax} \sum_{ic} \gamma_{ic}\left(\log \pi_{c} + \log \mathcal N (x_i; \mu_c, \Sigma_c)\right)
\end{aligned}
$$

In case of a  GMM, this can be done analytically by solving the following equations

$$
\begin{aligned}
    \nabla_{\mu_{c}} \sum_{ik} \gamma_{ik} \log \left(\mathcal N (x; \mu_k, \Sigma_k)\right) = 0
    \\ \nabla_{\Sigma_{c}} \sum_{ik} \gamma_{ik} \log \left(\mathcal N (x; \mu_k, \Sigma_k)\right) = 0
\end{aligned}
$$

Additionally the priors $$p(t_i = c) = \pi_c$$ need to be updated by solving

$$
    \nabla_{\nu} \left( \sum_{ic}  \gamma_{ic} \log \pi_c - \lambda \left(\sum_c \pi_c -1 \right)\right) = 0, \quad \nu = \pi_1, \pi_2, \pi_3, \lambda
$$

where the Lagrange multiplier ensures normalization of the weights $$\pi_c$$.

Finally, this leads to the following update formulas

$$
\begin{aligned}
    \pi_c = \frac{\sum_i \gamma_{ic}}{\sum_{ic} \gamma_{ic}} = \frac{1}{N}\sum_{i=1}^N \gamma_{ic} \\
    \mathbf \mu_c = \frac{\sum_{i=1}^N \gamma_{c,i} \mathbf{x}_i}{\sum_{i=1}^N \gamma_{c,i}} \\
    \Sigma_c = \frac{\sum_{i=1}^N \gamma_{c,i} (\mathbf{x}_i - \mathbf\mu_c) (\mathbf{x}_i - \mathbf{\mu}_1)^\top }{\sum_{i=1}^N \gamma_{c,i}}
\end{aligned}    
$$


#### Multivariate Gaussian PDF

$$
\begin{aligned}
    \mathcal N (x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d \det(\Sigma)}} \exp\left(-\frac 1 2 ({x}-{\mu})^\mathrm{T}{\Sigma}^{-1}({x}-{\mu})\right),\\
    x, \mu \in \mathbb R^d, \Sigma \in \mathbb R^{d\times d}
\end{aligned}
$$